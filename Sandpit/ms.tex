\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{dsfont}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[left=3cm,top=3cm,right=3cm]{geometry}

\newcommand{\xx}{\boldsymbol{x}}	% The unknown parameters
\newcommand{\data}{\mathbf{D}}  % The data
\newcommand{\dx}{d^N\mathbf{x}} % Volume element in parameter space
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\parindent=0cm

\title{}
\author{}

\begin{document}
\maketitle



\section{Analysis of a phase change}
The likelihood curve is $L(X)$. Defining $u = \ln(X)$ and $\ell = \ln L$, the likelihood is
then $L(u)$ (overloading $L$, so it remains the likelihood, not the same
function). The prior for $u$ is $-u \sim \textnormal{Expl}(1)$, i.e.

\begin{eqnarray}
\pi(u) &=& \exp(u)
\end{eqnarray}
and the canonical distribution at inverse temperature $\beta$ is
\begin{eqnarray}
p(u; \beta) &=& \frac{1}{Z(\beta)}\exp(u)L(u)^\beta
\end{eqnarray}

The posterior mode is
\begin{eqnarray}
\pi(u) &=& \exp(u)
\end{eqnarray}
and the canonical distribution at inverse temperature $\beta$ is
\begin{eqnarray}
p(u; \beta) &=& \frac{1}{Z(\beta)}\exp(u)L(u)^\beta
\end{eqnarray}

Taking the log of the posterior density:
\begin{eqnarray}
\ln p(u; \beta) &=& -\ln Z(\beta) + u + \beta \ell(u)
\end{eqnarray}

We can look for posterior modes (and antimodes!) by solving
$\frac{\partial}{\partial u} \ln p(u; \beta) = 0$, i.e.
\begin{eqnarray}
1 + \beta \frac{\partial \ell}{\partial u} &=& 0
\end{eqnarray}
Therefore modes and antimodes occur when
\begin{eqnarray}
\frac{\partial \ell}{\partial u} &=& -\frac{-1}{\beta}\\
&=& -T.
\end{eqnarray}





\newpage

WARNING: MOST OF THIS IS PROBABLY WRONG. I THINK I MISREMEMBERED THE
EQUATION FOR THE FISHER METRIC

Prior $\pi(\xx)$, likelihood $L(\xx)$.

\section{Canonical distributions}
Consider the partition function
\begin{eqnarray}
Z(\lambda) &=& \int L(\xx)^\lambda \pi(\xx) \, d\xx.
\end{eqnarray}
Suppose we want to evaluate $Z$ at a particular $\lambda$ value which is
unknown, and that our state of knowledge of $\lambda$ is the Jeffreys prior.
I'm just curious what this would imply. The conditional distributions of
interest are
\begin{eqnarray}
p(\xx | \lambda) &=& \frac{1}{Z(\lambda)}\pi(\xx)L(\xx)^\lambda.
\end{eqnarray}
and I'll take $L(\xx)$ as being the density with respect to background measure
$\pi$ (i.e. drop $\pi$ from the upcoming equations).
Let's start calculating the Jeffreys prior.
\begin{eqnarray}
\log p(\xx | \lambda) &=&  \lambda \log L(\xx) - \log Z(\lambda)
\end{eqnarray}
Taking the first partial derivative:
\begin{eqnarray}
\frac{\partial}{\partial \lambda}
\log p(\xx | \lambda) &=& \log L(\xx) - \frac{Z'(\lambda)}{Z(\lambda)}
\end{eqnarray}
and the second:
\begin{eqnarray}
\frac{\partial^2}{\partial \lambda^2}
\log p(\xx | \lambda) &=& \frac{Z'(\lambda)^2 - Z(\lambda)Z''(\lambda)}{Z(\lambda)^2}
\end{eqnarray}
which is rather messy and inelegant, so I'll stop there.

\section{Microcanonical distributions}
This will probably be more fruitful. Define a microcanonical distribution by
\begin{eqnarray}
p(\xx | \ell) &=& \frac{1}{Z(\ell)}\pi(\xx)\mathds{1}
\left(L(\xx) \in [l, l + h]\right)
\end{eqnarray}
Again, drop $\pi$. 


\section{A third idea}
Define the NS truncated distributions
\begin{eqnarray}
p(\xx | \ell) &=& \frac{1}{Z(\ell)}\pi(\xx)\mathds{1}
\left(L(\xx) > l\right)
\end{eqnarray}
Again, drop $\pi$. 


\end{document}

