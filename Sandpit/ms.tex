\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{dsfont}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[left=3cm,top=3cm,right=3cm]{geometry}

\newcommand{\xx}{\boldsymbol{x}}	% The unknown parameters
\newcommand{\data}{\mathbf{D}}  % The data
\newcommand{\dx}{d^N\mathbf{x}} % Volume element in parameter space
\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\parindent=0cm

\title{}
\author{}

\begin{document}
\maketitle



\section{Analysis of a phase change}
The likelihood curve is $L(X)$. Defining $u = \log(X)$ and $\ell = \log L$, the likelihood is
then $L(u)$ (overloading $L$, so it remains the likelihood, not the same
function). The prior for $u$ is $-u \sim \textnormal{Expl}(1)$, i.e.

\begin{eqnarray}
\pi(u) &=& \exp(u)
\end{eqnarray}
and the canonical distribution at inverse temperature $\beta$ is
\begin{eqnarray}
p(u; \beta) &=& \frac{1}{Z(\beta)}\exp(u)L(u)^\beta
\end{eqnarray}

The posterior mode is
\begin{eqnarray}
\pi(u) &=& \exp(u)
\end{eqnarray}
and the canonical distribution at inverse temperature $\beta$ is
\begin{eqnarray}
p(u; \beta) &=& \frac{1}{Z(\beta)}\exp(u)L(u)^\beta
\end{eqnarray}

Taking the log of the posterior density:
\begin{eqnarray}
\log p(u; \beta) &=& -\log Z(\beta) + u + \beta \ell(u)
\end{eqnarray}

We can look for posterior modes (and antimodes!) by solving
$\frac{\partial}{\partial u} \log p(u; \beta) = 0$, i.e.
\begin{eqnarray}
1 + \beta \frac{\partial \ell}{\partial u} &=& 0
\end{eqnarray}
Therefore modes and antimodes occur when
\begin{eqnarray}
\frac{\partial \ell}{\partial u} &=& -\frac{1}{\beta}\\
&=& -T.
\end{eqnarray}

The difficulty of jumping from one phase to another could be quantified by
the ratio of the maximum value of $p(u; \beta)$ at the maximum to the minimum.

\newpage

Prior $\pi(\xx)$, likelihood $L(\xx)$.

\section{Canonical distributions}
Consider the partition function
\begin{eqnarray}
Z(\lambda) &=& \int L(\xx)^\lambda \pi(\xx) \, d\xx.
\end{eqnarray}
Suppose we want to evaluate $Z$ at a particular $\lambda$ value which is
unknown, and that our state of knowledge of $\lambda$ is the Jeffreys prior.
I'm just curious what this would imply. The conditional distributions of
interest are
\begin{eqnarray}
p(\xx | \lambda) &=& \frac{1}{Z(\lambda)}\pi(\xx)L(\xx)^\lambda.
\end{eqnarray}
and I'll take $L(\xx)$ as being the density with respect to background measure
$\pi$ (i.e. drop $\pi$ from the upcoming equations).
Let's start calculating the Jeffreys prior.
\begin{eqnarray}
\log p(\xx | \lambda) &=&  \lambda \log L(\xx) - \log Z(\lambda)
\end{eqnarray}
Taking the first partial derivative:
\begin{eqnarray}
\frac{\partial}{\partial \lambda}
\log p(\xx | \lambda) &=&
\left[\log L(\xx)\right] - \frac{Z'(\lambda)}{Z(\lambda)}
\end{eqnarray}

Squaring this gives
\begin{eqnarray}
\left[\frac{\partial}{\partial \lambda}
\log p(\xx | \lambda)\right]^2 &=&
\left[\log L(\xx)\right]^2 - 2\frac{Z'(\lambda)}{Z(\lambda)}\log L(\xx)
+ \left[\frac{Z'(\lambda)}{Z(\lambda)}\right]^2
\end{eqnarray}


%and the second:
%\begin{eqnarray}
%\frac{\partial^2}{\partial \lambda^2}
%\log p(\xx | \lambda) &=& \frac{Z'(\lambda)^2 - Z(\lambda)Z''(\lambda)}{Z(\lambda)^2}
%\end{eqnarray}
%which is rather messy and inelegant, so I'll stop there.

\section{Microcanonical distributions}
This will probably be more fruitful. Define a microcanonical distribution by
\begin{eqnarray}
p(\xx | \ell) &=& \frac{1}{Z(\ell)}\pi(\xx)\mathds{1}
\left(L(\xx) \in [l, l + h]\right)
\end{eqnarray}
Again, drop $\pi$. 


\section{A third idea}
Define the NS truncated distributions
\begin{eqnarray}
p(\xx | \ell) &=& \frac{1}{Z(\ell)}\pi(\xx)\mathds{1}
\left(L(\xx) > l\right)
\end{eqnarray}
Again, drop $\pi$. 


\end{document}

